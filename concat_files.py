#!/usr/bin/env python3

import os
import argparse
import sys
import re

# Patterns for files and directories to ignore
IGNORE_PATTERNS = [
    # General directories to ignore
    r'(^|.*/)node_modules($|/)',    # Node.js dependencies
    r'(^|.*/)\.git($|/)',           # Git repo files
    r'(^|.*/)\.vscode($|/)',        # VSCode settings
    r'(^|.*/)\.idea($|/)',          # PyCharm project settings
    r'(^|.*/)\.roo($|/)',           # .roo/ configuration files
    r'(^|.*/)public($|/)',          # Static assets directory
    r'(^|.*/)output\.txt$',         # Output file generated by this script
    r'(^|.*/)prisma/dev\.db$',      # Development database file
    r'(^|.*/)\.DS_Store$',          # macOS Finder metadata
    r'(^|.*/)Thumbs\.db$',          # Windows thumbnail cache

    # Generated files
    r'(^|.*/)src/routeTree\.gen\.ts$',  # Auto-generated route tree

    # Common temporary / backup files
    r'\.pyc$', r'\.pyo$', r'\.pyd$',
    r'\.eslintcache$', r'\.nyc_output',
    r'\.tsbuildinfo$', r'\.bak$', r'\.tmp$', r'~$',
    
    # Data files not part of source code
    r'\.db$', r'\.sqlite$', r'\.sqlite3$', r'\.db-journal$',
    r'\.json$', r'\.geojson$', r'\.ndjson$',
    r'\.csv$', r'\.tsv$', r'\.txt$',

    # Build artifacts
    r'package-lock\.json$', r'yarn\.lock$',
    
    # Coverage, environment files
    r'\.coverage$', r'\.env$', r'\.pytest_cache',

    # Editor temp/swap files
    r'\.swp$', r'\.swo$',
]

def should_ignore(path):
    """
    Check if a file or directory should be ignored based on IGNORE_PATTERNS.
    """
    normalized = path.replace("\\", "/")  # Normalize Windows paths
    return any(re.search(pattern, normalized) for pattern in IGNORE_PATTERNS)

def process_directory(directory, max_depth=None, current_depth=0):
    """
    Recursively processes the directory to concatenate eligible files.
    """
    combined_content = ""
    file_sizes = []

    if max_depth is not None and current_depth > max_depth:
        return combined_content, file_sizes

    for entry in sorted(os.listdir(directory)):
        file_path = os.path.join(directory, entry)
        normalized_path = file_path.replace("\\", "/")

        if should_ignore(normalized_path):
            print(f"Ignoring: {normalized_path}")
            continue

        if os.path.isfile(file_path):
            try:
                header = f"======= {entry} ======= with the path {normalized_path}\n"
                combined_content += header
                with open(file_path, 'r', encoding='utf-8') as f:
                    combined_content += f.read() + "\n"

                file_size = os.path.getsize(file_path)
                file_sizes.append((normalized_path, file_size))

            except Exception as e:
                combined_content += f"Error reading file {normalized_path}: {e}\n"
        
        elif os.path.isdir(file_path):
            subdir_content, subdir_sizes = process_directory(file_path, max_depth, current_depth + 1)
            if subdir_content:
                combined_content += subdir_content
            file_sizes.extend(subdir_sizes)

    return combined_content, file_sizes

def main():
    parser = argparse.ArgumentParser(
        description="Concatenate all source files in a directory, skipping irrelevant or generated files."
    )
    parser.add_argument(
        "directory",
        type=str,
        help="Root directory to scan."
    )
    parser.add_argument(
        "--output",
        type=str,
        default="output.txt",
        help="Output file name (default: output.txt)"
    )
    parser.add_argument(
        "--max-depth",
        type=int,
        default=None,
        help="Maximum recursion depth (default: no limit)"
    )
    args = parser.parse_args()

    if not os.path.isdir(args.directory):
        print(f"Error: Directory '{args.directory}' does not exist.")
        sys.exit(1)

    combined_content, file_sizes = process_directory(args.directory, args.max_depth)

    try:
        with open(args.output, 'w', encoding='utf-8') as f:
            f.write(combined_content)
        print(f"Successfully wrote concatenated output to '{args.output}'.")
    except Exception as e:
        print(f"Error writing output file '{args.output}': {e}")
        sys.exit(1)

    if file_sizes:
        print("\nTop 5 largest processed files:")
        for path, size in sorted(file_sizes, key=lambda x: x[1], reverse=True)[:5]:
            print(f"{path}: {size / (1024 * 1024):.2f} MB")

if __name__ == "__main__":
    main()
